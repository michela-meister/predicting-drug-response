{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bfad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def candidate_split(df, ntrain):\n",
    "    p = df[['sample', 'drug']].drop_duplicates().reset_index(drop=True)\n",
    "    p = p.reindex(np.random.permutation(p.index))\n",
    "    train = p[:ntrain]\n",
    "    test = p[ntrain:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be37983",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '../results/2023-05-26/clean_and_split_data/welm_pdx_clean_mid_volume.csv'\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133faddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Drug.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d55f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_drugs = ['Fulvestrant (200 mg/kg)', 'Fulvestrant (40 mg/kg)']\n",
    "df.loc[df.Drug.isin(f_drugs)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42220123",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_drugs = {'Fulvestrant (200 mg/kg)': 'Fulvestrant', 'Fulvestrant (40 mg/kg)': 'Fulvestrant'}\n",
    "df['drug_collapsed'] = df['Drug'].replace(f_drugs)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1108e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Drug.unique())\n",
    "print(df.drug_collapsed.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b17c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "FRACTION_TRAIN = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f31fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_split(df, ntrain):\n",
    "    p = df[['sample', 'drug']].drop_duplicates().reset_index(drop=True)\n",
    "    p = p.reindex(np.random.permutation(p.index))\n",
    "    train = p[:ntrain]\n",
    "    test = p[ntrain:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2691c4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new candidate split\n",
    "\n",
    "def candidate_split(df, ntrain):\n",
    "    # Collapse Fulvestrant drugs\n",
    "    f_drugs = {'Fulvestrant (200 mg/kg)': 'Fulvestrant', 'Fulvestrant (40 mg/kg)': 'Fulvestrant'}\n",
    "    df['drug_collapsed'] = df['drug'].replace(f_drugs)\n",
    "    # Split on (sample, collapsed drug) pairs. This is to ensure that we don't end up with (sample A, Fulv 40mg) in\n",
    "    # the test set and (sample A, Fulv 200mg) in the training set, as an example.\n",
    "    p = df[['sample', 'drug_collapsed']].drop_duplicates().reset_index(drop=True)\n",
    "    p = p.reindex(np.random.permutation(p.index))\n",
    "    train_collapsed = p[:ntrain]\n",
    "    test_collapsed = p[ntrain:]\n",
    "    tups = df[['sample', 'drug', 'drug_collapsed']].drop_duplicates().reset_index(drop=True)\n",
    "    train_pairs = train_collapsed.merge(tups, on=['sample', 'drug_collapsed'], validate='one_to_many')\n",
    "    test_pairs = test_collapsed.merge(tups, on=['sample', 'drug_collapsed'], validate='one_to_many')\n",
    "    train_pairs = train_pairs[['sample', 'drug']].drop_duplicates()\n",
    "    test_pairs = test_pairs[['sample', 'drug']].drop_duplicates()\n",
    "    return train_pairs, test_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fddbd8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m vol_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog(V_V0+1)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m df \u001b[38;5;241m=\u001b[39m group_observations(df, vol_name)\n\u001b[0;32m---> 59\u001b[0m train, test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvol_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m validate_split(train, test, df)\n\u001b[1;32m     61\u001b[0m train\u001b[38;5;241m.\u001b[39mto_pickle(write_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/train.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36msplit_data\u001b[0;34m(df, vol_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_data\u001b[39m(df, vol_name):\n\u001b[0;32m---> 24\u001b[0m     train_pairs, test_pairs \u001b[38;5;241m=\u001b[39m \u001b[43msplit_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_drug_pair\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtuple\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     train \u001b[38;5;241m=\u001b[39m split_data_by_pairs(df, train_pairs, vol_name)\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36msplit_pairs\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     10\u001b[0m ntrain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mfloor(npairs \u001b[38;5;241m*\u001b[39m FRACTION_TRAIN))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     split \u001b[38;5;241m=\u001b[39m \u001b[43mcandidate_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_subset(split):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m split\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mcandidate_split\u001b[0;34m(df, ntrain)\u001b[0m\n\u001b[1;32m      9\u001b[0m p \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug_collapsed\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m p \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreindex(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(p\u001b[38;5;241m.\u001b[39mindex))\n\u001b[0;32m---> 11\u001b[0m train_collapsed \u001b[38;5;241m=\u001b[39m p[:\u001b[43mtrain\u001b[49m]\n\u001b[1;32m     12\u001b[0m test_collapsed \u001b[38;5;241m=\u001b[39m p[ntrain:]\n\u001b[1;32m     13\u001b[0m tups \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug_collapsed\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# checks if all the test values in column col appear in the training set\n",
    "def check_subset(split):\n",
    "    train, test = split\n",
    "    if ((set(test['sample']) <= set(train['sample'])) and (set(test['drug']) <= set(train['drug']))):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def split_pairs(df):\n",
    "    npairs = len(df[['sample', 'drug']].drop_duplicates())\n",
    "    ntrain = int(np.floor(npairs * FRACTION_TRAIN))\n",
    "    while True:\n",
    "        split = candidate_split(df, ntrain)\n",
    "        if check_subset(split):\n",
    "            return split\n",
    "        \n",
    "def split_data_by_pairs(df, pairs, vol_name):\n",
    "    pairs['sample_drug_pair'] = pairs[['sample', 'drug']].apply(tuple, axis=1)\n",
    "    pairs = pairs.merge(df, \n",
    "                        on=['sample', 'drug', 'sample_drug_pair'], \n",
    "                        validate='one_to_many')\n",
    "    return pairs[['sample', 'drug', vol_name + '_obs']]\n",
    "\n",
    "def split_data(df, vol_name):\n",
    "    train_pairs, test_pairs = split_pairs(df)\n",
    "    df['sample_drug_pair'] = df[['sample', 'drug']].apply(tuple, axis=1)\n",
    "    train = split_data_by_pairs(df, train_pairs, vol_name)\n",
    "    test = split_data_by_pairs(df, test_pairs, vol_name)\n",
    "    return train, test\n",
    "\n",
    "def validate_disjoint(train, test):\n",
    "    train_pairs = train[['sample', 'drug']].apply(tuple, axis=1)\n",
    "    test_pairs = test[['sample', 'drug']].apply(tuple, axis=1)\n",
    "    assert set(train_pairs).isdisjoint(set(test_pairs))\n",
    "    \n",
    "def validate_subset(train, test, col):\n",
    "    train_vals = train[col].unique()\n",
    "    test_vals = test[col].unique()\n",
    "    assert set(test_vals).issubset(set(train_vals))\n",
    "    \n",
    "def validate_length(train, test, df):\n",
    "    assert len(train) + len(test) == len(df)\n",
    "    \n",
    "def validate_split(train, test, df):\n",
    "    validate_length(train, test, df)\n",
    "    validate_disjoint(train, test)\n",
    "    validate_subset(train, test, 'sample')\n",
    "    validate_subset(train, test, 'drug')\n",
    "\n",
    "def group_observations(df, vol_name):\n",
    "    return df.groupby(['sample', 'drug'])[vol_name].apply(list).reset_index(name = vol_name + '_obs')\n",
    "\n",
    "read_fn = '../results/2023-05-26/clean_and_split_data/welm_pdx_clean_mid_volume.csv'\n",
    "df = pd.read_csv(read_fn)\n",
    "df = df[['Sample', 'Drug', 'log(V_V0+1)']]\n",
    "# map columns\n",
    "df = df.rename(columns={'Sample': 'sample', 'Drug': 'drug'})\n",
    "vol_name = 'log(V_V0+1)'\n",
    "df = group_observations(df, vol_name)\n",
    "train, test = split_data(df, vol_name)\n",
    "validate_split(train, test, df)\n",
    "train.to_pickle(write_dir + '/train.pkl')\n",
    "test.to_pickle(write_dir + '/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867b12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
